{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUJFDC9DcZ9a"
      },
      "source": [
        "# Mapreduce with bash\n",
        "\n",
        "In this notebook we're going to use `bash` to write a mapper and a reducer to count words in a file. This example will serve to illustrate the main features of Hadoop's MapReduce framework."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50IA8hcQcwmb",
        "outputId": "582becd1-f64f-4e50-d80d-22260f8d87bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r-WuKM3cyQq",
        "outputId": "cf7b18f0-a2c8-46ce-854a-909619c3a40f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCpql1nDczjK",
        "outputId": "cd43ece7-618e-4214-93e2-d034c74dfc67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMuEAo7OcZ9c"
      },
      "source": [
        "# Table of contents\n",
        "- [What is MapReduce?](#mapreduce)\n",
        "- [The mapper](#mapper)\n",
        "    - [Test the mapper](#testmapper)\n",
        "- [Hadoop it up](#hadoop)\n",
        "    - [What is Hadoop Streaming?](#hadoopstreaming)\n",
        "    - [List your Hadoop directory](#hdfs_ls)\n",
        "    - [Test MapReduce with a dummy reducer](#dummyreducer)\n",
        "    - [Shuffling and sorting](#shuffling&sorting)\n",
        "- [The reducer](#reducer)\n",
        "    - [Test and run](#run)\n",
        "- [Run a mapreduce job with more data](#moredata)\n",
        "    - [Sort the output with `sort`](#sortoutput)\n",
        "    - [Sort the output with another MapReduce job](#sortoutputMR)\n",
        "    - [Configure sort with `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n",
        "    - [Specifying Configuration Variables with the -D Option](#configuration_variables)\n",
        "    - [What is word count useful for?](#wordcount)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaoPwTjTcZ9c"
      },
      "source": [
        "## What is MapReduce? <a name=\"mapreduce\"></a>\n",
        "\n",
        "MapReduce is a computing paradigm designed to allow parallel distributed processing of massive amounts of data.\n",
        "\n",
        "Data is split across several computer nodes, there it is processed by one or more mappers. The results emitted by the mappers are first sorted and then passed to one or more reducers that process and combine the data to return the final result.\n",
        "\n",
        "![Map & Reduce](mapreduce.png)\n",
        "With [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) it is possible to use any programming language to define a mapper and/or a reducer. Here we're going to use the Unix `bash` scripting language ([here](https://www.gnu.org/software/bash/manual/html_node/index.html) is the official documentation for the language)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWigJEFgcZ9d"
      },
      "source": [
        "## The mapper <a name=\"mapper\"></a>\n",
        "Let's write a mapper script called `map.sh`. The mapper splits each input line into words and for each word it outputs a line containing the word and `1` separated by a tab.\n",
        "\n",
        "Example: for the input\n",
        "<html>\n",
        "<pre>\n",
        "apple orange\n",
        "banana apple peach\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "`map.sh` outputs:\n",
        "<html>\n",
        "<pre>\n",
        "apple   1\n",
        "orange  1\n",
        "banana  1\n",
        "apple  1\n",
        "peach  1\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "\n",
        "The <a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) allows us to write the contents of the cell to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8JJYJ7OcZ9d",
        "outputId": "870afa6f-6414-49e8-dc56-e9bdd6c83492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [ -n \"$word\" ]\n",
        "  then\n",
        "     echo -e ${word}\"\\t1\"\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUR_sbAgcZ9e"
      },
      "source": [
        "After running the cell above, you should have a new file `map.sh` in your current directory.\n",
        "The file can be seen in the left panel of JupyterLab or by using a list command on the bash command-line.\n",
        "\n",
        "**Note:** you can execute a single bash command in a Jupyter notebook cell by prepending an exclamation point to the command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RUOcoz6cZ9e",
        "outputId": "878b12cc-48c8-4761-ee7c-f81a7467f436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 126 Apr 24 00:14 map.sh\n"
          ]
        }
      ],
      "source": [
        "!ls -hl map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZkMDG8cZ9e"
      },
      "source": [
        "### Test the mapper <a name=\"testmapper\"></a>\n",
        "We're going to test the mapper on on the command line with a small text file `fruits.txt` by first creating the text file.\n",
        "In this file `apple` for instance appears two times, that's what we want our mapreduce job to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuPO_tYrcZ9e",
        "outputId": "d0577e0c-4bbf-477d-f6bb-4777b5d802df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fruits.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile fruits.txt\n",
        "apple banana\n",
        "peach orange peach peach\n",
        "pineapple peach apple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsSvhcvcZ9f",
        "outputId": "9c582a13-8ac9-47c3-b51c-e0530ac28850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple banana\n",
            "peach orange peach peach\n",
            "pineapple peach apple\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fib9TLhTcZ9f"
      },
      "source": [
        "Test the mapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neItLQCOcZ9f",
        "outputId": "170ee583-3f60-41bc-b90d-ef197b9938fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./map.sh: Permission denied\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eHYmnE_cZ9g"
      },
      "source": [
        "If the script `map.sh` does not have the executable bit set, you need to set the correct permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y2zoCkX4cZ9h"
      },
      "outputs": [],
      "source": [
        "!chmod 700 map.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHNt8-qcZ9h"
      },
      "source": [
        "## Hadoop it up <a name=\"hadoop\"></a>\n",
        "Let us now run a MapReduce job with Hadoop Streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFfXiesIcZ9h"
      },
      "source": [
        "### What is Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n",
        "\n",
        "Hadoop Streaming is a library included in the Hadoop distribution that enables you to develop MapReduce executables in languages other than Java.\n",
        "\n",
        "Mapper and/or reducer can be any sort of executables that read the input from stdin and emit the output to stdout. By default, input is read line by line and the prefix of a line up to the first tab character is the key; the rest of the line (excluding the tab character) will be the value.\n",
        "\n",
        "If there is no tab character in the line, then the entire line is considered as key and the value is null. The default input format is specified in the class `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) but this can can be customized for instance by defining another field separator (see the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n",
        "\n",
        "This is an example of MapReduce streaming invocation syntax:\n",
        "<html>\n",
        "<pre>\n",
        "    mapred streaming \\\n",
        "  -input myInputDirs \\\n",
        "  -output myOutputDir \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc\n",
        "\n",
        "</pre>\n",
        "</html>\n",
        "\n",
        "You can find the full official documentation for Hadoop Streaming from Apache Hadoop here: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n",
        "\n",
        "All options for the Hadoop Streaming command are described here: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options) and can be listed with the command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7gMkPIqcZ9h",
        "outputId": "1f6b667b-e38c-4dfb-b4f2-de3cff58f3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMD5lJwccZ9h"
      },
      "source": [
        "Now in order to run a mapreduce job that we need to \"upload\" the input file to the Hadoop file system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfHFqMcYcZ9h"
      },
      "source": [
        "### List your Hadoop directory <a name=\"hdfs_ls\"></a>\n",
        "\n",
        "With the command `hdfs dfs -l` you can view the content of your HDFS home directory.\n",
        "\n",
        "`hdfs dfs` you can run a filesystem command on the Hadoop fileystem. The complete list of commands can be found in the [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5acSydbocZ9h",
        "outputId": "ecad44c5-5a54-4659-9672-e1a4baa93d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6 items\n",
            "drwxr-xr-x   - root root       4096 2024-04-22 13:25 .config\n",
            "-rw-r--r--   1 root root         60 2024-04-24 00:14 fruits.txt\n",
            "drwxr-xr-x   - root root       4096 2024-03-04 08:05 hadoop-3.4.0\n",
            "-rw-r--r--   1 root root  965537117 2024-04-24 00:14 hadoop-3.4.0.tar.gz\n",
            "-rwx------   1 root root        126 2024-04-24 00:14 map.sh\n",
            "drwxr-xr-x   - root root       4096 2024-04-22 13:25 sample_data\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb-TIw76cZ9h"
      },
      "source": [
        "Now create a directory `wordcount` with a subdirectory `input` on the Hadoop filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K_Llq_WTcZ9h"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir -p wordcount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXBDdolacZ9h"
      },
      "source": [
        "Copy the file fruits.txt to Hadoop in the folder `wordcount/input`.\n",
        "\n",
        "Why do we need this step? Because the file `fruits.txt` needs to reside on the Hadoop filesystem in order to enjoy of all of the features of Hadoop (data partitioning, distributed processing, fault tolerance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4SrweqylcZ9i"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -mkdir wordcount/input\n",
        "hdfs dfs -put fruits.txt wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w3bkhZrcZ9i"
      },
      "source": [
        "Let's check if the file is there now.\n",
        "\n",
        "**Note:** it is convenient use the option `-h` for `ls` to show file sizes in human-readable form (showing sizes in Kilobytes, Megabytes, Gigabytes, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOUMhfT5cZ9i",
        "outputId": "c70efddb-089b-4705-a793-cb21b82fae58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root         60 2024-04-24 00:14 wordcount/input/fruits.txt\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls -h -R wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUAV_0jbcZ9i"
      },
      "source": [
        "### Test MapReduce with a dummy reducer <a name=\"dummyreducer\"></a>\n",
        "\n",
        "Let's try to run the mapper using a dummy reducer (`/bin/cat` does nothing else than echoing the data it receives).\n",
        "\n",
        "**Warning:** mapreduce tends to produce a verbose output, so be ready to see a long output. What you should look for is a message of the kind <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n",
        "\n",
        "**Note:** at the beginning of next cell you'll see a command `hadoop fs -rmr wordcount/output 2>/dev/null`. This is needed because when you run a job several times mapreduce will give an error if you try to overwrite the same output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ0ojz6RcZ9i",
        "outputId": "e479c1aa-0dc9-4131-bf3e-67038e711efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 00:15:00,942 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:15:01,170 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:15:01,171 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:15:01,196 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:01,569 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:15:01,594 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:15:02,045 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1317587774_0001\n",
            "2024-04-24 00:15:02,045 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:15:02,560 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1317587774_0001_3bced9e1-8152-4d69-97b5-7ded24bfa11d/map.sh\n",
            "2024-04-24 00:15:02,731 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:15:02,732 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:15:02,733 INFO mapreduce.Job: Running job: job_local1317587774_0001\n",
            "2024-04-24 00:15:02,737 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:15:02,746 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:02,746 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:02,811 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:15:02,819 INFO mapred.LocalJobRunner: Starting task: attempt_local1317587774_0001_m_000000_0\n",
            "2024-04-24 00:15:02,867 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:02,868 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:02,901 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:02,912 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-04-24 00:15:02,933 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:15:03,005 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:15:03,005 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:15:03,006 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:15:03,006 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:15:03,006 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:15:03,009 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:15:03,019 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-24 00:15:03,025 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:15:03,029 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:15:03,029 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:15:03,030 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:15:03,032 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:15:03,033 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:15:03,036 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:15:03,037 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:15:03,037 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:15:03,038 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:15:03,039 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:15:03,039 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:15:03,063 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:03,074 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-04-24 00:15:03,075 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:03,075 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:03,079 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:15:03,079 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:15:03,079 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:15:03,079 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-04-24 00:15:03,079 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-04-24 00:15:03,094 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:15:03,107 INFO mapred.Task: Task:attempt_local1317587774_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:03,111 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-04-24 00:15:03,111 INFO mapred.Task: Task 'attempt_local1317587774_0001_m_000000_0' done.\n",
            "2024-04-24 00:15:03,119 INFO mapred.Task: Final Counters for attempt_local1317587774_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142119\n",
            "\t\tFILE: Number of bytes written=861813\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=355467264\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-04-24 00:15:03,119 INFO mapred.LocalJobRunner: Finishing task: attempt_local1317587774_0001_m_000000_0\n",
            "2024-04-24 00:15:03,119 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:15:03,124 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:15:03,125 INFO mapred.LocalJobRunner: Starting task: attempt_local1317587774_0001_r_000000_0\n",
            "2024-04-24 00:15:03,172 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:03,172 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:03,173 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:03,177 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@277ba492\n",
            "2024-04-24 00:15:03,189 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:03,241 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:15:03,243 INFO reduce.EventFetcher: attempt_local1317587774_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:15:03,301 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1317587774_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-04-24 00:15:03,306 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local1317587774_0001_m_000000_0\n",
            "2024-04-24 00:15:03,309 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-04-24 00:15:03,312 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:15:03,314 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:03,314 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:15:03,325 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:03,325 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-24 00:15:03,327 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:15:03,327 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-04-24 00:15:03,328 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:15:03,328 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:03,329 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-24 00:15:03,330 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:03,331 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 00:15:03,336 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-24 00:15:03,338 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-24 00:15:03,355 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:03,364 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-04-24 00:15:03,365 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:03,365 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:03,367 INFO mapred.Task: Task:attempt_local1317587774_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:03,368 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:03,368 INFO mapred.Task: Task attempt_local1317587774_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:15:03,370 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1317587774_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-24 00:15:03,371 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-04-24 00:15:03,371 INFO mapred.Task: Task 'attempt_local1317587774_0001_r_000000_0' done.\n",
            "2024-04-24 00:15:03,372 INFO mapred.Task: Final Counters for attempt_local1317587774_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142355\n",
            "\t\tFILE: Number of bytes written=862005\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=355467264\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-04-24 00:15:03,372 INFO mapred.LocalJobRunner: Finishing task: attempt_local1317587774_0001_r_000000_0\n",
            "2024-04-24 00:15:03,372 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:15:03,741 INFO mapreduce.Job: Job job_local1317587774_0001 running in uber mode : false\n",
            "2024-04-24 00:15:03,743 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:15:03,744 INFO mapreduce.Job: Job job_local1317587774_0001 completed successfully\n",
            "2024-04-24 00:15:03,753 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=284474\n",
            "\t\tFILE: Number of bytes written=1723818\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=9\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=710934528\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=90\n",
            "2024-04-24 00:15:03,753 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -files map.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer /bin/cat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OGI8ZA3cZ9i"
      },
      "source": [
        "The output of the mapreduce job is in the `output` subfolder of the input directory. Let's check what's inside it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXzmULQlcZ9i",
        "outputId": "913c791a-2f54-452a-f02e-833f722a7297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 00:15 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root         78 2024-04-24 00:15 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niDzBitMcZ9i"
      },
      "source": [
        "If `output` contains a file named `_SUCCESS` that means that the mapreduce job completed successfully.\n",
        "\n",
        "**Note:** when dealing with Big Data it's always advisable to pipe the output of `cat` commands to `head` (or `tail`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbdT3i95cZ9i",
        "outputId": "e8531077-5e38-4121-9c2d-b70e957a16e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\t1\n",
            "apple\t1\n",
            "banana\t1\n",
            "orange\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "peach\t1\n",
            "pineapple\t1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStF2o6ncZ9j"
      },
      "source": [
        "We have gotten as expected all the output from the mapper. Something worth of notice is that the data outputted from the mapper _**has been sorted**_. We haven't asked for that but this step is automatically performed by the mapper as soon as the number of reducers is $\\gt 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2vrcEs0cZ9j"
      },
      "source": [
        "### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n",
        "The following picture illustrates the concept of shuffling and sorting that is automatically performed by Hadoop after each map before passing the output to reduce. In the picture the outputs of the two mapper tasks are shown. The arrows represent shuffling and sorting done before delivering the data to one reducer (rightmost box).\n",
        "![Shuffle & sort](shuffle_sort.png)\n",
        "The shuffling and sorting phase is often one of the most costly in a MapReduce job.\n",
        "\n",
        "\n",
        "<b>Note:</b> the job ran with two mappers because $2$ is the default number of mappers in Hadoop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo6XSxO_cZ9j"
      },
      "source": [
        "## The reducer <a name=\"reducer\"></a>\n",
        "Let's now write a reducer script called `reduce.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxAxFlRicZ9j",
        "outputId": "45da62fa-a627-4e62-b59c-3137252cc21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduce.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduce.sh\n",
        "#!/bin/bash\n",
        "\n",
        "currkey=\"\"\n",
        "currcount=0\n",
        "while IFS=$'\\t' read -r key val\n",
        "do\n",
        "  if [[ $key == $currkey ]]\n",
        "  then\n",
        "      currcount=$(( currcount + val ))\n",
        "  else\n",
        "    if [ -n \"$currkey\" ]\n",
        "    then\n",
        "      echo -e ${currkey} \"\\t\" ${currcount}\n",
        "    fi\n",
        "    currkey=$key\n",
        "    currcount=1\n",
        "  fi\n",
        "done\n",
        "# last one\n",
        "echo -e ${currkey} \"\\t\" ${currcount}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4LqAui6cZ9j"
      },
      "source": [
        "Set permission for the reducer script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4tOLu-JvcZ9j"
      },
      "outputs": [],
      "source": [
        "!chmod 700 reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGHHFQpscZ9j"
      },
      "source": [
        "### Test and run <a name=\"run\"></a>\n",
        "\n",
        "Test map and reduce on the shell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp4GY33TcZ9k",
        "outputId": "372198cb-cc2d-4847-d5de-d015dec30683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat fruits.txt|./map.sh|sort|./reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3IJtAvcZ9k"
      },
      "source": [
        "Once we've made sure that the reducer script runs correctly on the shell, we can run it on the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eek_v0dWcZ9k",
        "outputId": "c6494587-7eae-49bd-a07f-e7d71e24716d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob12941580087132467633.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 00:15:12,438 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-24 00:15:13,277 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:15:13,440 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:15:13,440 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:15:13,469 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:13,734 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:15:13,763 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:15:14,105 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local861479966_0001\n",
            "2024-04-24 00:15:14,106 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:15:14,603 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local861479966_0001_a4b5f294-0f05-4c1e-836c-a8f2835df964/map.sh\n",
            "2024-04-24 00:15:14,636 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local861479966_0001_506ae0d7-6a38-41e2-a312-cae12dc4fa99/reduce.sh\n",
            "2024-04-24 00:15:14,809 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:15:14,811 INFO mapreduce.Job: Running job: job_local861479966_0001\n",
            "2024-04-24 00:15:14,818 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:15:14,820 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:15:14,836 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:14,836 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:14,900 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:15:14,913 INFO mapred.LocalJobRunner: Starting task: attempt_local861479966_0001_m_000000_0\n",
            "2024-04-24 00:15:14,963 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:14,966 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:14,999 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:15,010 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n",
            "2024-04-24 00:15:15,027 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:15:15,112 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:15:15,112 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:15:15,112 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:15:15,112 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:15:15,112 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:15:15,116 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:15:15,124 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-24 00:15:15,133 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:15:15,136 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:15:15,137 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:15:15,137 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:15:15,138 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:15:15,138 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:15:15,140 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:15:15,140 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:15:15,141 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:15:15,141 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:15:15,142 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:15:15,143 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:15:15,168 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:15,178 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:15,178 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2024-04-24 00:15:15,179 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:15,182 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:15:15,182 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:15:15,182 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:15:15,182 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
            "2024-04-24 00:15:15,183 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
            "2024-04-24 00:15:15,192 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:15:15,212 INFO mapred.Task: Task:attempt_local861479966_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:15,214 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2024-04-24 00:15:15,215 INFO mapred.Task: Task 'attempt_local861479966_0001_m_000000_0' done.\n",
            "2024-04-24 00:15:15,237 INFO mapred.Task: Final Counters for attempt_local861479966_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1200\n",
            "\t\tFILE: Number of bytes written=716563\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=9\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=375390208\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "2024-04-24 00:15:15,237 INFO mapred.LocalJobRunner: Finishing task: attempt_local861479966_0001_m_000000_0\n",
            "2024-04-24 00:15:15,237 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:15:15,242 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:15:15,242 INFO mapred.LocalJobRunner: Starting task: attempt_local861479966_0001_r_000000_0\n",
            "2024-04-24 00:15:15,269 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:15,269 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:15,270 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:15,281 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@78c14739\n",
            "2024-04-24 00:15:15,292 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:15,326 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:15:15,331 INFO reduce.EventFetcher: attempt_local861479966_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:15:15,392 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local861479966_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2024-04-24 00:15:15,396 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local861479966_0001_m_000000_0\n",
            "2024-04-24 00:15:15,401 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2024-04-24 00:15:15,404 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:15:15,406 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:15,406 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:15:15,416 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:15,416 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-24 00:15:15,418 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:15:15,419 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2024-04-24 00:15:15,419 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:15:15,419 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:15,420 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n",
            "2024-04-24 00:15:15,421 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:15,427 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-04-24 00:15:15,430 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-24 00:15:15,433 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-24 00:15:15,453 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:15,457 INFO streaming.PipeMapRed: Records R/W=9/1\n",
            "2024-04-24 00:15:15,458 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:15,458 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:15,460 INFO mapred.Task: Task:attempt_local861479966_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:15,461 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:15,462 INFO mapred.Task: Task attempt_local861479966_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:15:15,464 INFO output.FileOutputCommitter: Saved output of task 'attempt_local861479966_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-24 00:15:15,465 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
            "2024-04-24 00:15:15,465 INFO mapred.Task: Task 'attempt_local861479966_0001_r_000000_0' done.\n",
            "2024-04-24 00:15:15,466 INFO mapred.Task: Final Counters for attempt_local861479966_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1436\n",
            "\t\tFILE: Number of bytes written=716733\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=9\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=375390208\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-04-24 00:15:15,466 INFO mapred.LocalJobRunner: Finishing task: attempt_local861479966_0001_r_000000_0\n",
            "2024-04-24 00:15:15,466 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:15:15,817 INFO mapreduce.Job: Job job_local861479966_0001 running in uber mode : false\n",
            "2024-04-24 00:15:15,818 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:15:15,820 INFO mapreduce.Job: Job job_local861479966_0001 completed successfully\n",
            "2024-04-24 00:15:15,839 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2636\n",
            "\t\tFILE: Number of bytes written=1433296\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=9\n",
            "\t\tMap output bytes=78\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=92\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=9\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=18\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=750780416\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=76\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=68\n",
            "2024-04-24 00:15:15,839 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tcDo4ZYcZ9k"
      },
      "source": [
        "Let's check the output on the HDFS filesystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NTHrTWOcZ9k",
        "outputId": "b017df7b-0f16-4990-8a40-f6cc9fb33607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple \t 2\n",
            "banana \t 1\n",
            "orange \t 1\n",
            "peach \t 4\n",
            "pineapple \t 1\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part*|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHjgDA3bcZ9k"
      },
      "source": [
        "## Run a mapreduce job with more data <a name=\"moredata\"></a>\n",
        "\n",
        "Let's create a datafile by downloading some real data, for instance from a Web page. This example will be used to introduce some advanced configurations.\n",
        "\n",
        "Next, we download a URL with `wget` and filter out HTML tags with a `sed` regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KBvaOLI3cZ9k"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n",
        "wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OCipedVcZ9k",
        "outputId": "9b70acf9-41c8-43c1-c326-9ce5d13f06df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "\r\t1\n",
            "Und\t1\n",
            "wo\t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toPKplxhcZ9l"
      },
      "source": [
        "As usual, with real data there's some more work to do. Here we see that the mapper script doesn't skip empty lines. Let's modify it so that empty lines are skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaEGTMLfcZ9l",
        "outputId": "b096a2e4-d777-4c6a-f79b-a4e84ccf28b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting map.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile map.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while read line\n",
        "do\n",
        " for word in $line\n",
        " do\n",
        "  if [[ \"$line\" =~ [^[:space:]] ]]\n",
        "  then\n",
        "    if [ -n \"$word\" ]\n",
        "    then\n",
        "    echo -e ${word} \"\\t1\"\n",
        "    fi\n",
        "  fi\n",
        " done\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyY9YiUdcZ9l",
        "outputId": "683347d8-e4af-4537-9c63-3b2e47bb8866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t1\n",
            "Und \t1\n",
            "wo \t1\n",
            "warst \t1\n",
            "du \t1\n",
            "beim \t1\n",
            "Fall \t1\n",
            "der \t1\n",
            "Mauer? \t1\n",
            "- \t1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99aslnFtcZ9l"
      },
      "source": [
        "Now the output of `map.sh` looks better!\n",
        "\n",
        "<b>Note:</b> when working with real data we need in general some more preprocessing in order to remove control characters or invalid unicode.\n",
        "\n",
        "Time to run MapReduce again with the new data, but first we need to \"put\" the data on HDFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxM4UH92cZ9l",
        "outputId": "1514baf8-f3bc-460e-be41-f5ff8a336972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/input\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/input 2>/dev/null\n",
        "hdfs dfs -put sample_article.txt wordcount/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE8b7SWdcZ9l",
        "outputId": "b37c8b96-e764-4d42-924a-43857b48162a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r--   1 root root     29.0 K 2024-04-24 00:15 wordcount/input\n"
          ]
        }
      ],
      "source": [
        "# check that the folder wordcount/input on HDFS only contains sample_article.txt\n",
        "!hdfs dfs -ls -h wordcount/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiV_tcHLcZ9m"
      },
      "source": [
        "Check the reducer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg5z8JmVcZ9m",
        "outputId": "764b8255-aa69-4386-fe81-74036eb50fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t 1\n",
            "Und \t 1\n",
            "wo \t 1\n",
            "warst \t 1\n",
            "du \t 1\n",
            "beim \t 1\n",
            "Fall \t 1\n",
            "der \t 1\n",
            "Mauer? \t 1\n",
            "- \t 1\n"
          ]
        }
      ],
      "source": [
        "!cat sample_article.txt|./map.sh|./reduce.sh|head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LE8j6SbcZ9m",
        "outputId": "cb978739-4be0-498b-c7ea-99722de63935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output\n",
            "packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob16992136986853921105.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 00:15:28,790 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-24 00:15:29,996 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:15:30,268 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:15:30,268 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:15:30,304 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:30,665 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:15:30,726 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:15:31,320 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local902789587_0001\n",
            "2024-04-24 00:15:31,321 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:15:32,150 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local902789587_0001_de9c259f-138f-4818-8965-3a50841f3b34/map.sh\n",
            "2024-04-24 00:15:32,214 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local902789587_0001_33c67799-b36a-45c3-8cd1-3c335b34d7d5/reduce.sh\n",
            "2024-04-24 00:15:32,500 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:15:32,502 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:15:32,509 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:15:32,521 INFO mapreduce.Job: Running job: job_local902789587_0001\n",
            "2024-04-24 00:15:32,536 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:32,539 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:32,632 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:15:32,642 INFO mapred.LocalJobRunner: Starting task: attempt_local902789587_0001_m_000000_0\n",
            "2024-04-24 00:15:32,729 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:32,730 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:32,777 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:32,799 INFO mapred.MapTask: Processing split: file:/content/wordcount/input:0+29720\n",
            "2024-04-24 00:15:32,832 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:15:32,927 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:15:32,927 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:15:32,927 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:15:32,928 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:15:32,928 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:15:32,933 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:15:32,945 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n",
            "2024-04-24 00:15:32,965 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:15:32,967 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:15:32,975 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:15:32,975 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:15:32,976 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:15:32,976 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:15:32,978 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:15:32,978 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:15:32,978 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:15:32,979 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:15:32,979 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:15:32,988 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:15:33,035 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,036 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,038 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,048 INFO streaming.PipeMapRed: Records R/W=186/1\n",
            "2024-04-24 00:15:33,380 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:33,381 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:33,385 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:15:33,385 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:15:33,385 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:15:33,385 INFO mapred.MapTask: bufstart = 0; bufend = 32527; bufvoid = 104857600\n",
            "2024-04-24 00:15:33,385 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209816(104839264); length = 4581/6553600\n",
            "2024-04-24 00:15:33,417 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:15:33,438 INFO mapred.Task: Task:attempt_local902789587_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:33,443 INFO mapred.LocalJobRunner: Records R/W=186/1\n",
            "2024-04-24 00:15:33,443 INFO mapred.Task: Task 'attempt_local902789587_0001_m_000000_0' done.\n",
            "2024-04-24 00:15:33,468 INFO mapred.Task: Final Counters for attempt_local902789587_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=31161\n",
            "\t\tFILE: Number of bytes written=751403\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1146\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=392167424\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "2024-04-24 00:15:33,468 INFO mapred.LocalJobRunner: Finishing task: attempt_local902789587_0001_m_000000_0\n",
            "2024-04-24 00:15:33,468 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:15:33,483 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:15:33,484 INFO mapred.LocalJobRunner: Starting task: attempt_local902789587_0001_r_000000_0\n",
            "2024-04-24 00:15:33,510 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:33,513 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:33,514 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:33,519 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7c23920d\n",
            "2024-04-24 00:15:33,521 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:33,539 INFO mapreduce.Job: Job job_local902789587_0001 running in uber mode : false\n",
            "2024-04-24 00:15:33,542 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-24 00:15:33,545 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:15:33,548 INFO reduce.EventFetcher: attempt_local902789587_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:15:33,602 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local902789587_0001_m_000000_0 decomp: 34869 len: 34873 to MEMORY\n",
            "2024-04-24 00:15:33,617 INFO reduce.InMemoryMapOutput: Read 34869 bytes from map-output for attempt_local902789587_0001_m_000000_0\n",
            "2024-04-24 00:15:33,623 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34869, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34869\n",
            "2024-04-24 00:15:33,625 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:15:33,626 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:33,626 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:15:33,637 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:33,637 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-04-24 00:15:33,647 INFO reduce.MergeManagerImpl: Merged 1 segments, 34869 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:15:33,648 INFO reduce.MergeManagerImpl: Merging 1 files, 34873 bytes from disk\n",
            "2024-04-24 00:15:33,649 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:15:33,649 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:33,650 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n",
            "2024-04-24 00:15:33,651 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:33,661 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n",
            "2024-04-24 00:15:33,665 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-24 00:15:33,668 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-24 00:15:33,688 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,688 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,690 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,699 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:33,708 INFO streaming.PipeMapRed: Records R/W=1146/1\n",
            "2024-04-24 00:15:33,778 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:33,780 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:33,783 INFO mapred.Task: Task:attempt_local902789587_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:33,784 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:33,784 INFO mapred.Task: Task attempt_local902789587_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:15:33,790 INFO output.FileOutputCommitter: Saved output of task 'attempt_local902789587_0001_r_000000_0' to file:/content/wordcount/output\n",
            "2024-04-24 00:15:33,792 INFO mapred.LocalJobRunner: Records R/W=1146/1 > reduce\n",
            "2024-04-24 00:15:33,793 INFO mapred.Task: Task 'attempt_local902789587_0001_r_000000_0' done.\n",
            "2024-04-24 00:15:33,795 INFO mapred.Task: Final Counters for attempt_local902789587_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=100939\n",
            "\t\tFILE: Number of bytes written=815868\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1146\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=392167424\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-04-24 00:15:33,795 INFO mapred.LocalJobRunner: Finishing task: attempt_local902789587_0001_r_000000_0\n",
            "2024-04-24 00:15:33,796 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:15:34,545 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:15:34,546 INFO mapreduce.Job: Job job_local902789587_0001 completed successfully\n",
            "2024-04-24 00:15:34,556 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=132100\n",
            "\t\tFILE: Number of bytes written=1567271\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=186\n",
            "\t\tMap output records=1146\n",
            "\t\tMap output bytes=32527\n",
            "\t\tMap output materialized bytes=34873\n",
            "\t\tInput split bytes=81\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=736\n",
            "\t\tReduce shuffle bytes=34873\n",
            "\t\tReduce input records=1146\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=2292\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=784334848\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29968\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=29592\n",
            "2024-04-24 00:15:34,556 INFO streaming.StreamJob: Output directory: wordcount/output\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hadoop fs -rmr wordcount/output 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file map.sh \\\n",
        "  -file reduce.sh \\\n",
        "  -input wordcount/input \\\n",
        "  -output wordcount/output \\\n",
        "  -mapper map.sh \\\n",
        "  -reducer reduce.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4pWfq08cZ9m"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klQOTweGcZ9m",
        "outputId": "bb197be0-4f24-4313-fd27-de71a7fd3500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 00:15 wordcount/output/_SUCCESS\n",
            "-rw-r--r--   1 root root      29352 2024-04-24 00:15 wordcount/output/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcv-5xVBcZ9m"
      },
      "source": [
        "This job took a few seconds and this is quite some time for such a small file (4KB). This is due to the overhead of distributing the data and running the Hadoop framework.\n",
        "The advantage of Hadoop can be appreciated only for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdMLkvJncZ9m",
        "outputId": "50a4b5b9-c982-487f-c83c-56969d58f770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!n.frames[t]; \t 1\n",
            "!0)); \t 1\n",
            "!1)) \t 1\n",
            "!1, \t 1\n",
            "!= \t 1\n",
            "!== \t 2\n",
            "!function \t 2\n",
            "!r \t 1\n",
            "\"'+n+'\"',o)}return{key:r,value:e.substr(t+1)}},t._renewCache=function(){t._cache=t._getCacheFromString(t._document.cookie),t._cachedDocumentCookie=t._document.cookie},t._areEnabled=function(){var \t 1\n",
            "\"))}function \t 1\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jox3UwescZ9n"
      },
      "source": [
        "### Sort the output with `sort` <a name=\"sortoutput\"></a>\n",
        "\n",
        "We've obtained a list of tokens that appear in the file followed by their frequencies.\n",
        "\n",
        "The output of the reducer is sorted by key (the word) because that's the ordering that the reducer becomes from the mapper. If we're interested in sorting the data by frequency, we can use the Unix `sort` command (with the options `k2`, `n`, `r` respectively \"by field 2\", \"numeric\", \"reverse\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knu9lMvEcZ9n",
        "outputId": "e0ecac20-178e-4f97-92e0-133f881ba634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= \t 40\n",
            "{ \t 22\n",
            "var \t 22\n",
            "&& \t 19\n",
            "strict\";function \t 13\n",
            "} \t 12\n",
            "in \t 12\n",
            "not \t 12\n",
            "to \t 10\n",
            "e&&e.__esModule?e:{\"default\":e}}function \t 9\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usGiyUlOcZ9n"
      },
      "source": [
        "The most common word appears to be \"die\" (the German for the definite article \"the\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1f2mtsLcZ9n"
      },
      "source": [
        "### Sort the output with another MapReduce job <a name=\"sortoutputMR\"></a>\n",
        "\n",
        "If we wanted to sort the output of the reducer using the mapreduce framework, we could employ a simple trick: create a mapper that interchanges words with their frequency values. Since by construction mappers sort their output by key, we get the desired sorting as a side-effect.\n",
        "\n",
        "Call the new mapper `swap_keyval.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dSWH46vcZ9n",
        "outputId": "756df7c7-19f9-4f92-9d9e-d380721eba55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing swap_keyval.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile swap_keyval.sh\n",
        "#!/bin/bash\n",
        "# This script will read one line at a time and swap key/value\n",
        "# For instance, the line \"word 100\" will become \"100 word\"\n",
        "\n",
        "while read key val\n",
        "do\n",
        " printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvCo2Y-VcZ9n"
      },
      "source": [
        "We are going to run the swap mapper script on the output of the previous mapreduce job. Note that in the below cell we are not deleting the previous output but instead we're saving the output from the current job in a new folder `output_sorted`.\n",
        "\n",
        "Nice thing about running a job on the output of a preceding job is that we do not need to upload files to HDFS because the data is already on HDFS. Not so nice: writing data to disk at each step of a data transformation pipeline takes time and this can be costly for longer data pipelines. This is one of the shortcomings of MapReduce that are addressed by [Apache Spark](https://spark.apache.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrPCekO2cZ9n",
        "outputId": "ef9c82f0-260a-4228-fbfc-34fd518ee4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob1348949727193682736.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 00:15:44,231 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-24 00:15:45,568 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:15:46,077 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:15:46,077 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:15:46,139 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:46,409 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:15:46,435 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:15:46,823 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1772727373_0001\n",
            "2024-04-24 00:15:46,823 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:15:47,292 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local1772727373_0001_ac08b27b-0e65-4f30-b446-0862a5017e31/swap_keyval.sh\n",
            "2024-04-24 00:15:47,440 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:15:47,442 INFO mapreduce.Job: Running job: job_local1772727373_0001\n",
            "2024-04-24 00:15:47,448 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:15:47,451 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:15:47,456 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:47,457 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:47,521 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:15:47,526 INFO mapred.LocalJobRunner: Starting task: attempt_local1772727373_0001_m_000000_0\n",
            "2024-04-24 00:15:47,584 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:47,584 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:47,619 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:47,629 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-04-24 00:15:47,645 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:15:47,723 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:15:47,723 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:15:47,723 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:15:47,723 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:15:47,723 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:15:47,727 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:15:47,734 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-04-24 00:15:47,740 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:15:47,744 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:15:47,744 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:15:47,745 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:15:47,746 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:15:47,747 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:15:47,751 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:15:47,752 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:15:47,753 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:15:47,754 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:15:47,755 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:15:47,756 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:15:47,782 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:47,783 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:47,784 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:15:47,804 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-04-24 00:15:47,861 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:15:47,861 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:15:47,864 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:15:47,865 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:15:47,865 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:15:47,865 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-04-24 00:15:47,865 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-04-24 00:15:47,880 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:15:47,905 INFO mapred.Task: Task:attempt_local1772727373_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:47,908 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-04-24 00:15:47,908 INFO mapred.Task: Task 'attempt_local1772727373_0001_m_000000_0' done.\n",
            "2024-04-24 00:15:47,920 INFO mapred.Task: Final Counters for attempt_local1772727373_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30237\n",
            "\t\tFILE: Number of bytes written=747014\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-04-24 00:15:47,920 INFO mapred.LocalJobRunner: Finishing task: attempt_local1772727373_0001_m_000000_0\n",
            "2024-04-24 00:15:47,921 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:15:47,947 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:15:47,956 INFO mapred.LocalJobRunner: Starting task: attempt_local1772727373_0001_r_000000_0\n",
            "2024-04-24 00:15:47,969 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:15:47,970 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:15:47,972 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:15:47,976 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7bd9196a\n",
            "2024-04-24 00:15:47,978 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:15:47,998 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:15:48,011 INFO reduce.EventFetcher: attempt_local1772727373_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:15:48,051 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1772727373_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-04-24 00:15:48,055 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local1772727373_0001_m_000000_0\n",
            "2024-04-24 00:15:48,060 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-04-24 00:15:48,064 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:15:48,065 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:48,065 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:15:48,072 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:48,072 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-04-24 00:15:48,080 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:15:48,081 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-04-24 00:15:48,082 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:15:48,082 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:15:48,083 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n",
            "2024-04-24 00:15:48,083 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:48,129 INFO mapred.Task: Task:attempt_local1772727373_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:15:48,133 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:15:48,133 INFO mapred.Task: Task attempt_local1772727373_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:15:48,136 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1772727373_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-04-24 00:15:48,142 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 00:15:48,143 INFO mapred.Task: Task 'attempt_local1772727373_0001_r_000000_0' done.\n",
            "2024-04-24 00:15:48,144 INFO mapred.Task: Final Counters for attempt_local1772727373_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89175\n",
            "\t\tFILE: Number of bytes written=804555\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-24 00:15:48,145 INFO mapred.LocalJobRunner: Finishing task: attempt_local1772727373_0001_r_000000_0\n",
            "2024-04-24 00:15:48,145 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:15:48,447 INFO mapreduce.Job: Job job_local1772727373_0001 running in uber mode : false\n",
            "2024-04-24 00:15:48,448 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:15:48,449 INFO mapreduce.Job: Job job_local1772727373_0001 completed successfully\n",
            "2024-04-24 00:15:48,459 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119412\n",
            "\t\tFILE: Number of bytes written=1551569\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=11\n",
            "\t\tTotal committed heap usage (bytes)=864026624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-24 00:15:48,459 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r wordcount/output2 2>/dev/null\n",
        "mapred streaming \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4ubxMCQcZ9o"
      },
      "source": [
        "Check the output on HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYI62M7bcZ9o",
        "outputId": "3129fbff-85a3-4ab0-c1b3-67e2e76b9e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 00:15 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-04-24 00:15 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWD3YrfKcZ9o",
        "outputId": "e4fa61af-8872-4399-ee00-f4285a03c46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t!!n.frames[t];\n",
            "1\tberraschen.\n",
            "1\tber\n",
            "1\t\n",
            "1\t},\n",
            "1\t}();\n",
            "1\t}(),\n",
            "1\t{};\n",
            "1\ty(){E[\"default\"].debug(\"User\n",
            "1\ty(),j(),void(ne=D());case\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWwfJXkIcZ9o"
      },
      "source": [
        "Mapper uses by default ascending order to sort by key. We could have changed that with an option but for now let's look at the end of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRgSwZNPcZ9o",
        "outputId": "9b410215-f486-4882-c49f-e17c3f813168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\t==\n",
            "7\t:\n",
            "7\tdie\n",
            "7\t?\n",
            "7\tif\n",
            "7\ttypeof\n",
            "8\t0\n",
            "8\tn(e){return\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "9\tr\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|tail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR-szt7tcZ9o"
      },
      "source": [
        "### Configure sort with `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n",
        "\n",
        "In general, we can determine how mappers are going to sort their output by configuring the comparator directive to use the special class [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n",
        "<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n",
        "    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n",
        "    \n",
        "This class has some options similar to the Unix `sort`(`-n` to sort numerically, `-r` for reverse sorting, `-k pos1[,pos2]` for specifying fields to sort by).\n",
        "\n",
        "Let us see the comparator in action on our data to get the desired result. Note that this time we are removing `output2` because we're running the second mapreduce job again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQEC16YPcZ9o",
        "outputId": "334bda56-02d1-4950-e93a-59ffe736677e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted wordcount/output2\n",
            "packageJobJar: [swap_keyval.sh] [] /tmp/streamjob4559690087618625575.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 00:15:58,923 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2024-04-24 00:15:59,841 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:16:00,030 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:16:00,030 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:16:00,054 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:16:00,346 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:16:00,376 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:16:00,696 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local997550504_0001\n",
            "2024-04-24 00:16:00,696 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:16:01,185 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local997550504_0001_78c8f001-2352-470b-87d4-156913eda453/swap_keyval.sh\n",
            "2024-04-24 00:16:01,316 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:16:01,317 INFO mapreduce.Job: Running job: job_local997550504_0001\n",
            "2024-04-24 00:16:01,324 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:16:01,326 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:16:01,332 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:16:01,332 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:16:01,389 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:16:01,393 INFO mapred.LocalJobRunner: Starting task: attempt_local997550504_0001_m_000000_0\n",
            "2024-04-24 00:16:01,446 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:16:01,446 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:16:01,466 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:16:01,476 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n",
            "2024-04-24 00:16:01,492 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:16:01,584 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:16:01,584 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:16:01,585 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:16:01,585 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:16:01,585 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:16:01,591 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:16:01,604 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n",
            "2024-04-24 00:16:01,619 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:16:01,623 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:16:01,624 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:16:01,625 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:16:01,626 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:16:01,627 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:16:01,631 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:16:01,631 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:16:01,632 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:16:01,632 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:16:01,633 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:16:01,634 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:16:01,666 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:16:01,667 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:16:01,668 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:16:01,694 INFO streaming.PipeMapRed: Records R/W=746/1\n",
            "2024-04-24 00:16:01,737 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:16:01,738 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:16:01,741 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:16:01,741 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:16:01,742 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:16:01,742 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n",
            "2024-04-24 00:16:01,742 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n",
            "2024-04-24 00:16:01,761 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:16:01,780 INFO mapred.Task: Task:attempt_local997550504_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:16:01,783 INFO mapred.LocalJobRunner: Records R/W=746/1\n",
            "2024-04-24 00:16:01,783 INFO mapred.Task: Task 'attempt_local997550504_0001_m_000000_0' done.\n",
            "2024-04-24 00:16:01,804 INFO mapred.Task: Final Counters for attempt_local997550504_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=30237\n",
            "\t\tFILE: Number of bytes written=744453\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=746\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "2024-04-24 00:16:01,804 INFO mapred.LocalJobRunner: Finishing task: attempt_local997550504_0001_m_000000_0\n",
            "2024-04-24 00:16:01,808 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:16:01,813 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:16:01,814 INFO mapred.LocalJobRunner: Starting task: attempt_local997550504_0001_r_000000_0\n",
            "2024-04-24 00:16:01,838 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:16:01,838 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:16:01,838 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:16:01,843 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@fad6feb\n",
            "2024-04-24 00:16:01,847 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:16:01,878 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:16:01,897 INFO reduce.EventFetcher: attempt_local997550504_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:16:01,977 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local997550504_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n",
            "2024-04-24 00:16:01,981 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local997550504_0001_m_000000_0\n",
            "2024-04-24 00:16:01,984 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n",
            "2024-04-24 00:16:01,989 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:16:01,990 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:16:01,990 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:16:02,000 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:16:02,000 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-04-24 00:16:02,021 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:16:02,022 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n",
            "2024-04-24 00:16:02,023 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:16:02,023 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:16:02,024 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n",
            "2024-04-24 00:16:02,025 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:16:02,065 INFO mapred.Task: Task:attempt_local997550504_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:16:02,066 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:16:02,066 INFO mapred.Task: Task attempt_local997550504_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:16:02,068 INFO output.FileOutputCommitter: Saved output of task 'attempt_local997550504_0001_r_000000_0' to file:/content/wordcount/output2\n",
            "2024-04-24 00:16:02,070 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 00:16:02,070 INFO mapred.Task: Task 'attempt_local997550504_0001_r_000000_0' done.\n",
            "2024-04-24 00:16:02,071 INFO mapred.Task: Final Counters for attempt_local997550504_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=89175\n",
            "\t\tFILE: Number of bytes written=801994\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=746\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-24 00:16:02,071 INFO mapred.LocalJobRunner: Finishing task: attempt_local997550504_0001_r_000000_0\n",
            "2024-04-24 00:16:02,071 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:16:02,323 INFO mapreduce.Job: Job job_local997550504_0001 running in uber mode : false\n",
            "2024-04-24 00:16:02,324 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:16:02,325 INFO mapreduce.Job: Job job_local997550504_0001 completed successfully\n",
            "2024-04-24 00:16:02,335 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=119412\n",
            "\t\tFILE: Number of bytes written=1546447\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=746\n",
            "\t\tMap output records=746\n",
            "\t\tMap output bytes=27907\n",
            "\t\tMap output materialized bytes=29453\n",
            "\t\tInput split bytes=93\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15\n",
            "\t\tReduce shuffle bytes=29453\n",
            "\t\tReduce input records=746\n",
            "\t\tReduce output records=746\n",
            "\t\tSpilled Records=1492\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=715128832\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29596\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28088\n",
            "2024-04-24 00:16:02,335 INFO streaming.StreamJob: Output directory: wordcount/output2\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "hdfs dfs -rmr wordcount/output2 2>/dev/null\n",
        "comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n",
        "mapred streaming \\\n",
        "  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n",
        "  -D mapreduce.partition.keycomparator.options=-nr \\\n",
        "  -file swap_keyval.sh \\\n",
        "  -input wordcount/output \\\n",
        "  -output wordcount/output2 \\\n",
        "  -mapper swap_keyval.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-wncVDicZ9p",
        "outputId": "e09fe975-1dbc-4e04-b737-7277ed1da666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 00:16 wordcount/output2/_SUCCESS\n",
            "-rw-r--r--   1 root root      27860 2024-04-24 00:16 wordcount/output2/part-00000\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -ls wordcount/output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6lP03D1cZ9p",
        "outputId": "f873a592-07e5-480e-9cf5-0986bb5a75bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\t=\n",
            "22\t{\n",
            "22\tvar\n",
            "19\t&&\n",
            "13\tstrict\";function\n",
            "12\tnot\n",
            "12\t}\n",
            "12\tin\n",
            "10\tto\n",
            "9\te&&e.__esModule?e:{\"default\":e}}function\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat wordcount/output2/part-00000|head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MJU6Tq9cZ9p"
      },
      "source": [
        "Now we get the output in the desired order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGm7kQblcZ9p"
      },
      "source": [
        "### Specifying Configuration Variables with the -D Option <a name=\"configuration_variables\"></a>\n",
        "\n",
        "With the `-D` option it is possible to override options set in the default configuration file [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n",
        "(see the [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n",
        "\n",
        "One option that might come handy when dealing with out-of-memory issues in the sorting phase is the size in MB of the memory reserved for sorting `mapreduce.task.io.sort.mb`:\n",
        " <html>\n",
        "    <pre>-D mapreduce.task.io.sort.mb=512\n",
        "    </pre>\n",
        " </html>\n",
        "\n",
        " **Note:** the maximum value for `mapreduce.task.io.sort.mb` is 2047.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rovwCXRMcZ9p"
      },
      "source": [
        "## What is word count useful for? <a name=\"wordcount\"></a>\n",
        "Counting the frequencies of words is at the basis of _indexing_ and it facilitates the retrieval of relevant documents in search engines."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}