{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "b6d67559-97a6-4df8-be21-4e5f59191c7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "16be0d1f-45b2-40b5-b38e-f78d210891c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "7fe48132-53b2-459b-c863-b4b7fcc44042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "c5af20b3-a7f4-4f73-ced1-d0c3ef87a0cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-04-23 23:33:36,930 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 23:33:37,201 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 23:33:37,201 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 23:33:37,224 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:33:37,585 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 23:33:37,616 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 23:33:38,051 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1221811281_0001\n",
            "2024-04-23 23:33:38,052 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 23:33:38,289 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 23:33:38,291 INFO mapreduce.Job: Running job: job_local1221811281_0001\n",
            "2024-04-23 23:33:38,300 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 23:33:38,308 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 23:33:38,315 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:38,316 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:38,371 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 23:33:38,379 INFO mapred.LocalJobRunner: Starting task: attempt_local1221811281_0001_m_000000_0\n",
            "2024-04-23 23:33:38,428 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:38,434 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:38,477 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:33:38,489 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:33:38,509 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 23:33:38,598 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 23:33:38,598 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 23:33:38,598 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 23:33:38,598 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 23:33:38,598 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 23:33:38,603 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 23:33:38,608 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 23:33:38,623 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 23:33:38,626 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 23:33:38,627 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 23:33:38,627 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 23:33:38,628 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 23:33:38,628 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 23:33:38,630 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 23:33:38,630 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 23:33:38,631 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 23:33:38,631 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 23:33:38,632 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 23:33:38,633 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 23:33:38,664 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 23:33:38,670 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 23:33:38,669 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 23:33:38,671 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 23:33:38,706 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:33:38,707 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 23:33:38,707 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 23:33:38,707 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-23 23:33:38,707 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 23:33:38,719 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 23:33:38,735 INFO mapred.Task: Task:attempt_local1221811281_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:33:38,739 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 23:33:38,740 INFO mapred.Task: Task 'attempt_local1221811281_0001_m_000000_0' done.\n",
            "2024-04-23 23:33:38,749 INFO mapred.Task: Final Counters for attempt_local1221811281_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=857635\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=30\n",
            "\t\tTotal committed heap usage (bytes)=396361728\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 23:33:38,749 INFO mapred.LocalJobRunner: Finishing task: attempt_local1221811281_0001_m_000000_0\n",
            "2024-04-23 23:33:38,750 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 23:33:38,756 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 23:33:38,756 INFO mapred.LocalJobRunner: Starting task: attempt_local1221811281_0001_r_000000_0\n",
            "2024-04-23 23:33:38,768 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:38,768 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:38,769 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:33:38,776 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50b73b4c\n",
            "2024-04-23 23:33:38,778 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:33:38,805 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 23:33:38,816 INFO reduce.EventFetcher: attempt_local1221811281_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 23:33:38,866 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1221811281_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-23 23:33:38,876 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1221811281_0001_m_000000_0\n",
            "2024-04-23 23:33:38,881 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-23 23:33:38,885 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 23:33:38,887 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:38,887 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 23:33:38,913 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 23:33:38,913 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 23:33:38,922 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 23:33:38,927 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-23 23:33:38,928 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 23:33:38,934 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 23:33:38,936 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 23:33:38,937 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:38,949 INFO mapred.Task: Task:attempt_local1221811281_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:33:38,951 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:38,951 INFO mapred.Task: Task attempt_local1221811281_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 23:33:38,954 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1221811281_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 23:33:38,955 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 23:33:38,956 INFO mapred.Task: Task 'attempt_local1221811281_0001_r_000000_0' done.\n",
            "2024-04-23 23:33:38,957 INFO mapred.Task: Final Counters for attempt_local1221811281_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=857685\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=396361728\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 23:33:38,957 INFO mapred.LocalJobRunner: Finishing task: attempt_local1221811281_0001_r_000000_0\n",
            "2024-04-23 23:33:38,957 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 23:33:39,297 INFO mapreduce.Job: Job job_local1221811281_0001 running in uber mode : false\n",
            "2024-04-23 23:33:39,298 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 23:33:39,300 INFO mapreduce.Job: Job job_local1221811281_0001 completed successfully\n",
            "2024-04-23 23:33:39,316 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1715320\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=30\n",
            "\t\tTotal committed heap usage (bytes)=792723456\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 23:33:39,316 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "dd31b4ee-c179-452b-c8be-cb9ba01f874a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "db7e978d-2d42-47ad-e984-d16534784b90"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-23 23:33 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-23 23:33 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "8424a5ba-bfee-4296-88ec-5a78d083b168"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 23 23:33 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 23 23:33 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "e1a6e7c1-8875-448e-dfbe-adc22f9b56fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "d3a92760-6a33-46e1-8110-3e47185c903c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 23:33:46,844 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "c8699ea8-15f9-453b-dbf6-e248fb592536"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 23:33:48,941 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 23:33:51,571 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 23:33:51,784 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 23:33:51,785 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 23:33:51,818 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:33:52,156 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 23:33:52,210 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 23:33:52,602 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1221593932_0001\n",
            "2024-04-23 23:33:52,602 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 23:33:52,904 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 23:33:52,906 INFO mapreduce.Job: Running job: job_local1221593932_0001\n",
            "2024-04-23 23:33:52,920 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 23:33:52,927 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 23:33:52,939 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:52,939 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:53,010 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 23:33:53,021 INFO mapred.LocalJobRunner: Starting task: attempt_local1221593932_0001_m_000000_0\n",
            "2024-04-23 23:33:53,083 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:53,085 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:53,131 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:33:53,150 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:33:53,174 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 23:33:53,272 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 23:33:53,272 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 23:33:53,272 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 23:33:53,273 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 23:33:53,273 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 23:33:53,280 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 23:33:53,291 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:33:53,292 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 23:33:53,292 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 23:33:53,292 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-23 23:33:53,292 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 23:33:53,303 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 23:33:53,323 INFO mapred.Task: Task:attempt_local1221593932_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:33:53,328 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:33:53,328 INFO mapred.Task: Task 'attempt_local1221593932_0001_m_000000_0' done.\n",
            "2024-04-23 23:33:53,340 INFO mapred.Task: Final Counters for attempt_local1221593932_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 23:33:53,341 INFO mapred.LocalJobRunner: Finishing task: attempt_local1221593932_0001_m_000000_0\n",
            "2024-04-23 23:33:53,343 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 23:33:53,348 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 23:33:53,354 INFO mapred.LocalJobRunner: Starting task: attempt_local1221593932_0001_r_000000_0\n",
            "2024-04-23 23:33:53,369 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:33:53,370 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:33:53,370 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:33:53,379 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f16232a\n",
            "2024-04-23 23:33:53,382 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:33:53,409 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 23:33:53,428 INFO reduce.EventFetcher: attempt_local1221593932_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 23:33:53,474 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1221593932_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-23 23:33:53,479 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1221593932_0001_m_000000_0\n",
            "2024-04-23 23:33:53,485 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-23 23:33:53,489 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 23:33:53,491 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:53,491 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 23:33:53,503 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 23:33:53,504 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 23:33:53,506 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 23:33:53,507 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-23 23:33:53,508 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 23:33:53,508 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 23:33:53,513 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 23:33:53,514 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:53,525 INFO mapred.Task: Task:attempt_local1221593932_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:33:53,527 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 23:33:53,529 INFO mapred.Task: Task attempt_local1221593932_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 23:33:53,532 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1221593932_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 23:33:53,536 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 23:33:53,536 INFO mapred.Task: Task 'attempt_local1221593932_0001_r_000000_0' done.\n",
            "2024-04-23 23:33:53,539 INFO mapred.Task: Final Counters for attempt_local1221593932_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=855583\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 23:33:53,539 INFO mapred.LocalJobRunner: Finishing task: attempt_local1221593932_0001_r_000000_0\n",
            "2024-04-23 23:33:53,539 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 23:33:53,917 INFO mapreduce.Job: Job job_local1221593932_0001 running in uber mode : false\n",
            "2024-04-23 23:33:53,918 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 23:33:53,920 INFO mapreduce.Job: Job job_local1221593932_0001 completed successfully\n",
            "2024-04-23 23:33:53,945 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1711108\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=790626304\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 23:33:53,946 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "09eee86f-ee46-41bd-e556-159ccc0ee269"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "731a381b-8b69-4378-cbda-ff181897b6b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "f2451eba-f5f9-49a8-f675-724234d1db6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 23:33:59,265 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 23:34:01,788 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 23:34:02,055 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 23:34:02,055 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 23:34:02,095 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:34:02,493 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 23:34:02,540 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 23:34:02,948 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1436452999_0001\n",
            "2024-04-23 23:34:02,948 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 23:34:03,304 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 23:34:03,310 INFO mapreduce.Job: Running job: job_local1436452999_0001\n",
            "2024-04-23 23:34:03,325 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 23:34:03,333 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 23:34:03,343 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:34:03,343 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:34:03,433 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 23:34:03,442 INFO mapred.LocalJobRunner: Starting task: attempt_local1436452999_0001_m_000000_0\n",
            "2024-04-23 23:34:03,518 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:34:03,518 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:34:03,547 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:34:03,562 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:34:03,585 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 23:34:03,612 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:34:03,628 INFO mapred.Task: Task:attempt_local1436452999_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:34:03,630 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:34:03,630 INFO mapred.Task: Task attempt_local1436452999_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 23:34:03,633 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1436452999_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 23:34:03,635 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:34:03,635 INFO mapred.Task: Task 'attempt_local1436452999_0001_m_000000_0' done.\n",
            "2024-04-23 23:34:03,646 INFO mapred.Task: Final Counters for attempt_local1436452999_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=423624704\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 23:34:03,647 INFO mapred.LocalJobRunner: Finishing task: attempt_local1436452999_0001_m_000000_0\n",
            "2024-04-23 23:34:03,650 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 23:34:04,331 INFO mapreduce.Job: Job job_local1436452999_0001 running in uber mode : false\n",
            "2024-04-23 23:34:04,334 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 23:34:04,354 INFO mapreduce.Job: Job job_local1436452999_0001 completed successfully\n",
            "2024-04-23 23:34:04,365 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=423624704\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 23:34:04,366 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "fe4bc03c-5c13-4354-b806-4a6014677817"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "bc457a37-a40b-4491-c490-d68303b80e61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 23:34:08,620 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 23:34:12,723 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 23:34:12,973 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 23:34:12,973 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 23:34:13,002 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 23:34:13,380 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 23:34:13,424 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 23:34:13,817 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local454967744_0001\n",
            "2024-04-23 23:34:13,817 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 23:34:14,133 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 23:34:14,136 INFO mapreduce.Job: Running job: job_local454967744_0001\n",
            "2024-04-23 23:34:14,149 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 23:34:14,152 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 23:34:14,164 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:34:14,165 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:34:14,269 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 23:34:14,278 INFO mapred.LocalJobRunner: Starting task: attempt_local454967744_0001_m_000000_0\n",
            "2024-04-23 23:34:14,327 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 23:34:14,329 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 23:34:14,366 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 23:34:14,379 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 23:34:14,400 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 23:34:14,417 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 23:34:14,425 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 23:34:14,428 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 23:34:14,428 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 23:34:14,429 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 23:34:14,429 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 23:34:14,430 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 23:34:14,432 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 23:34:14,432 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 23:34:14,433 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 23:34:14,433 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 23:34:14,434 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 23:34:14,435 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 23:34:14,458 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 23:34:14,462 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 23:34:14,463 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 23:34:14,463 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 23:34:14,468 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:34:14,482 INFO mapred.Task: Task:attempt_local454967744_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 23:34:14,484 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 23:34:14,484 INFO mapred.Task: Task attempt_local454967744_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 23:34:14,497 INFO output.FileOutputCommitter: Saved output of task 'attempt_local454967744_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 23:34:14,499 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 23:34:14,502 INFO mapred.Task: Task 'attempt_local454967744_0001_m_000000_0' done.\n",
            "2024-04-23 23:34:14,513 INFO mapred.Task: Final Counters for attempt_local454967744_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=414187520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 23:34:14,513 INFO mapred.LocalJobRunner: Finishing task: attempt_local454967744_0001_m_000000_0\n",
            "2024-04-23 23:34:14,515 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 23:34:15,145 INFO mapreduce.Job: Job job_local454967744_0001 running in uber mode : false\n",
            "2024-04-23 23:34:15,147 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 23:34:15,150 INFO mapreduce.Job: Job job_local454967744_0001 completed successfully\n",
            "2024-04-23 23:34:15,159 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=414187520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 23:34:15,159 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "ef030f5c-13e8-417c-d09f-1532c3cad66c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}