{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)\n",
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))\n",
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cbjcsXoeIUa",
        "outputId": "eb2407e2-bbca-4b15-8296-65b4fce0db4c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n",
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y59n_nBOd87z"
      },
      "source": [
        "# Simple distributed wordcount with MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTb_j70rd870"
      },
      "source": [
        "Check that file `file.txt` exists, view size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2hw8tRYd871",
        "outputId": "1d3c7243-0d34-49c4-ed7b-5fbc2fb50a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 348 Apr 24 00:45 file.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -hal file.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRc6B38Rd871"
      },
      "source": [
        "Copy file to HDFS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hjsFsWCgd871"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -put -f file.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onx9QlBXd872"
      },
      "source": [
        "Erase `result` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "elm88xjPd872"
      },
      "outputs": [],
      "source": [
        "!hdfs dfs -rm -R result 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tepprHw9d872"
      },
      "source": [
        "Run the bash wordcount command `wc` in parallel on the distributed file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azfq6JmNd872",
        "outputId": "8a3d11a8-1c24-4898-a334-21231b4955d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-24 00:46:34,755 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 00:46:35,121 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 00:46:35,121 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 00:46:35,144 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:46:35,488 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 00:46:35,544 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 00:46:35,967 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local138415833_0001\n",
            "2024-04-24 00:46:35,967 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 00:46:36,216 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 00:46:36,218 INFO mapreduce.Job: Running job: job_local138415833_0001\n",
            "2024-04-24 00:46:36,228 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 00:46:36,234 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 00:46:36,244 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:46:36,244 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:46:36,311 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 00:46:36,320 INFO mapred.LocalJobRunner: Starting task: attempt_local138415833_0001_m_000000_0\n",
            "2024-04-24 00:46:36,412 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:46:36,413 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:46:36,464 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:46:36,493 INFO mapred.MapTask: Processing split: file:/content/file.txt:0+348\n",
            "2024-04-24 00:46:36,513 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 00:46:36,601 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 00:46:36,601 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 00:46:36,601 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 00:46:36,601 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 00:46:36,601 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 00:46:36,607 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 00:46:36,614 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 00:46:36,629 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 00:46:36,633 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 00:46:36,634 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 00:46:36,634 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 00:46:36,635 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 00:46:36,636 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 00:46:36,640 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 00:46:36,641 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 00:46:36,641 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 00:46:36,642 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 00:46:36,643 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 00:46:36,644 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 00:46:36,686 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:46:36,686 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:46:36,689 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 00:46:36,689 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:46:36,692 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 00:46:36,692 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 00:46:36,692 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 00:46:36,692 INFO mapred.MapTask: bufstart = 0; bufend = 352; bufvoid = 104857600\n",
            "2024-04-24 00:46:36,692 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-24 00:46:36,701 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 00:46:36,727 INFO mapred.Task: Task:attempt_local138415833_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:46:36,733 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-24 00:46:36,733 INFO mapred.Task: Task 'attempt_local138415833_0001_m_000000_0' done.\n",
            "2024-04-24 00:46:36,746 INFO mapred.Task: Final Counters for attempt_local138415833_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142269\n",
            "\t\tFILE: Number of bytes written=856195\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=352\n",
            "\t\tMap output materialized bytes=362\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=368\n",
            "2024-04-24 00:46:36,746 INFO mapred.LocalJobRunner: Finishing task: attempt_local138415833_0001_m_000000_0\n",
            "2024-04-24 00:46:36,747 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 00:46:36,752 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 00:46:36,752 INFO mapred.LocalJobRunner: Starting task: attempt_local138415833_0001_r_000000_0\n",
            "2024-04-24 00:46:36,763 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 00:46:36,763 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 00:46:36,764 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 00:46:36,768 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@125526ab\n",
            "2024-04-24 00:46:36,770 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 00:46:36,809 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 00:46:36,816 INFO reduce.EventFetcher: attempt_local138415833_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 00:46:36,872 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local138415833_0001_m_000000_0 decomp: 358 len: 362 to MEMORY\n",
            "2024-04-24 00:46:36,878 INFO reduce.InMemoryMapOutput: Read 358 bytes from map-output for attempt_local138415833_0001_m_000000_0\n",
            "2024-04-24 00:46:36,883 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 358, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->358\n",
            "2024-04-24 00:46:36,886 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 00:46:36,887 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:46:36,887 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 00:46:36,895 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:46:36,896 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 00:46:36,897 INFO reduce.MergeManagerImpl: Merged 1 segments, 358 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 00:46:36,898 INFO reduce.MergeManagerImpl: Merging 1 files, 362 bytes from disk\n",
            "2024-04-24 00:46:36,899 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 00:46:36,899 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 00:46:36,900 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 00:46:36,900 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:46:36,902 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2024-04-24 00:46:36,906 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-04-24 00:46:36,912 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-04-24 00:46:36,931 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 00:46:36,941 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 00:46:36,943 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 00:46:36,943 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 00:46:36,945 INFO mapred.Task: Task:attempt_local138415833_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 00:46:36,947 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 00:46:36,947 INFO mapred.Task: Task attempt_local138415833_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 00:46:36,950 INFO output.FileOutputCommitter: Saved output of task 'attempt_local138415833_0001_r_000000_0' to file:/content/result\n",
            "2024-04-24 00:46:36,958 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
            "2024-04-24 00:46:36,959 INFO mapred.Task: Task 'attempt_local138415833_0001_r_000000_0' done.\n",
            "2024-04-24 00:46:36,959 INFO mapred.Task: Final Counters for attempt_local138415833_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=143025\n",
            "\t\tFILE: Number of bytes written=856594\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=362\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=37\n",
            "2024-04-24 00:46:36,960 INFO mapred.LocalJobRunner: Finishing task: attempt_local138415833_0001_r_000000_0\n",
            "2024-04-24 00:46:36,961 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 00:46:37,225 INFO mapreduce.Job: Job job_local138415833_0001 running in uber mode : false\n",
            "2024-04-24 00:46:37,226 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 00:46:37,227 INFO mapreduce.Job: Job job_local138415833_0001 completed successfully\n",
            "2024-04-24 00:46:37,242 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=285294\n",
            "\t\tFILE: Number of bytes written=1712789\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=352\n",
            "\t\tMap output materialized bytes=362\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=362\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=864026624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=368\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=37\n",
            "2024-04-24 00:46:37,242 INFO streaming.StreamJob: Output directory: result\n"
          ]
        }
      ],
      "source": [
        "!mapred streaming \\\n",
        "  -input file.txt \\\n",
        "  -output result \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv8Zs3EQd872"
      },
      "source": [
        "Check result of MapReduce job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBt8I9wDd872",
        "outputId": "bde34189-cd96-41fd-8252-96631274a218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      1     116     350\t\n"
          ]
        }
      ],
      "source": [
        "!hdfs dfs -cat result/part*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7avjGVXd872"
      },
      "source": [
        "Check that the word count is correct by comparing with `wc` on local host (warning: do not try with too large files)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlDIBOzEd873",
        "outputId": "70f1fdb9-0508-4331-b84d-b8a2e5cfd895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 116 348 file.txt\n"
          ]
        }
      ],
      "source": [
        "!wc file.txt"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}